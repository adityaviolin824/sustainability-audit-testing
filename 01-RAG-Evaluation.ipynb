{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c721e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval_and_response_eval.eval import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = test.load_tests() # we get the list of testing jsons to pass into model ig [make with chatgpt, refine with gemini]\n",
    "len(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What product does the IIOTY award winner work on?\n",
      "spanning\n",
      "Maxine Thompson, who won the IIOTY award in 2023, works as a Senior Data Engineer.\n",
      "['Maxine', 'Thompson', 'Senior Data Engineer', 'IIOTY']\n"
     ]
    }
   ],
   "source": [
    "example = tests[100] \n",
    "# life becomes a lot easier as we convert it into pydantic rather than a dictionary/json, so don't have to use []\n",
    "\n",
    "print(example.question)\n",
    "print(example.category)\n",
    "print(example.reference_answer)\n",
    "print(example.keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94bb2a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'direct_fact': 70,\n",
       "         'temporal': 20,\n",
       "         'spanning': 20,\n",
       "         'comparative': 10,\n",
       "         'numerical': 10,\n",
       "         'relationship': 10,\n",
       "         'holistic': 10})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create a Counter that counts how many times each test category appears\n",
    "# We extract the `category` field from every object in `tests`\n",
    "count = Counter([t.category for t in tests])\n",
    "\n",
    "# `count` is now a dictionary-like object:\n",
    "# keys   -> category names\n",
    "# values -> number of tests in each category\n",
    "count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.eval import evaluate_retrieval, evaluate_answer \n",
    "# rag inference and overall answer inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalEval(mrr=0.25, ndcg=0.41032085947766006, keywords_found=3, total_keywords=4, keyword_coverage=75.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to Debug this, not sure why answers are so poor [RAG Metrics] fixed\n",
    "\n",
    "evaluate_retrieval(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Qualitative Metrics] given by LLM \n",
    "\n",
    "eval, answer, chunks = evaluate_answer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33559c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerEval(feedback='The answer provides detailed information about the projects Maxine is involved in, which adds context but does not specify the exact product associated with the IIOTY award. The reference is very minimal, only stating her role as a Senior Data Engineer. While the generated answer appears plausible, it introduces specific product details not confirmed by the reference or question. Overall, it attempts to address the question but includes assumptions and lacks direct confirmation of the exact product.', accuracy=3.0, completeness=3.0, relevance=3.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83578c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer provides detailed information about the projects Maxine is involved in, which adds context but does not specify the exact product associated with the IIOTY award. The reference is very minimal, only stating her role as a Senior Data Engineer. While the generated answer appears plausible, it introduces specific product details not confirmed by the reference or question. Overall, it attempts to address the question but includes assumptions and lacks direct confirmation of the exact product.\n",
      "3.0\n",
      "3.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "print(eval.feedback)\n",
    "print(eval.accuracy)\n",
    "print(eval.completeness)\n",
    "print(eval.relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b710b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
